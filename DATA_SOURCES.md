# Multiple Data Source Support

The BA Review Application now supports loading data from **three different sources**: Excel, SQLite, and Snowflake.

## Quick Start

The application uses `datasources.json` to manage multiple data sources. You can **switch between sources using the dropdown in the navbar** without restarting the application!

### UI-Based Switching (Recommended)
1. Configure all your data sources in `datasources.json`
2. Use the **Data Source** dropdown in the navbar
3. Select a source and confirm the switch
4. Application reloads with new data automatically

### File-Based Switching (Legacy)
Alternatively, edit `datasources.json` to change the `active` field and restart the app.

## Configuration

### Excel (Default)

```json
{
  "source_type": "excel",
  "file_path": "C:\\ClaudeMain\\BA_Dedup2\\BA_Dedup2\\output\\canvas_dec_matches.xlsx"
}
```

**Use when:**
- Working with local Excel files
- Data is generated by the matching pipeline
- Quick testing and development

---

### SQLite

```json
{
  "source_type": "sqlite",
  "db_path": "C:\\ClaudeMain\\BA_Dedup2\\BA_Dedup2\\canvas_dec_matches.db",
  "table_name": "canvas_dec_matches"
}
```

**Use when:**
- Need faster query performance
- Working with larger datasets
- Want to avoid Excel file locking issues
- Multiple users accessing data (with proper connection handling)

**Setup:**
To create a SQLite database from your Excel file:

```python
import pandas as pd
import sqlite3

# Load Excel
df = pd.read_excel('canvas_dec_matches.xlsx')

# Save to SQLite
conn = sqlite3.connect('canvas_dec_matches.db')
df.to_sql('canvas_dec_matches', conn, if_exists='replace', index=False)
conn.close()
```

---

### Snowflake

```json
{
  "source_type": "snowflake",
  "account": "your_account",
  "user": "your_user",
  "password": "your_password",
  "database": "your_database",
  "schema": "your_schema",
  "table": "canvas_dec_matches",
  "warehouse": "your_warehouse"
}
```

**Use when:**
- Working with enterprise data warehouse
- Need cloud-based storage and compute
- Sharing data across teams
- Want advanced features (time travel, cloning, etc.)

**Setup:**
1. Install Snowflake connector:
   ```bash
   pip install snowflake-connector-python
   ```

2. Create table in Snowflake (from Python):
   ```python
   import pandas as pd
   from snowflake import connector
   from snowflake.connector.pandas_tools import write_pandas

   # Load data
   df = pd.read_excel('canvas_dec_matches.xlsx')

   # Connect to Snowflake
   conn = connector.connect(
       account='your_account',
       user='your_user',
       password='your_password',
       database='your_database',
       schema='your_schema',
       warehouse='your_warehouse'
   )

   # Write data
   success, nchunks, nrows, _ = write_pandas(
       conn, df, 'CANVAS_DEC_MATCHES', auto_create_table=True
   )

   conn.close()
   ```

---

## Switching Data Sources

### Method 1: Frontend UI (Recommended)

1. **Click the Data Source dropdown** in the navbar (top-right)
2. **Select your desired source** from the list
3. **Confirm the switch** when prompted
4. **Application reloads automatically** with new data

Features:
- ✅ No restart required
- ✅ Instant switching
- ✅ Visual confirmation
- ✅ Shows record count after switch

### Method 2: Configuration File (Legacy)

1. **Edit datasources.json**
   - Change the `active` field to your desired source ID
   ```json
   {
     "datasources": { ... },
     "active": "snowflake"
   }
   ```

2. **Restart the Flask application**
   ```bash
   python app.py
   ```

3. **Verify in startup banner**
   ```
   ==============================================================
     BA DEDUPLICATION REVIEW APPLICATION
   ==============================================================
     Data Source: SNOWFLAKE
     Account: your_account
     Database: your_database.your_schema.canvas_dec_matches
     Records loaded: 1,234
   ```

---

## Data Requirements

All data sources must provide a DataFrame with these columns:

### Required Columns:
- `canvas_id` - Canvas identifier
- `canvas_ssn` - Canvas SSN
- `canvas_name` - Canvas name
- `canvas_address` - Canvas address
- `canvas_city` - Canvas city
- `canvas_state` - Canvas state
- `canvas_zip` - Canvas zip
- `dec_hdrcode` - DEC header code
- `dec_name` - DEC name
- `dec_address` - DEC address
- `dec_city` - DEC city
- `dec_state` - DEC state
- `dec_zip` - DEC zip
- `dec_contact` - DEC contact
- `ssn_match` - SSN match score (0-100)
- `name_score` - Name match score (0-100)
- `address_score` - Address match score (0-100)
- `recommendation` - Match recommendation
- `address_reason` - Address reason

### Optional Columns:
- `jib` - JIB flag (0 or 1)
- `rev` - Rev flag (0 or 1)
- `vendor` - Vendor flag (0 or 1)

The data loader will automatically add missing `jib`, `rev`, and `vendor` columns with default value 0.

---

## Performance Comparison

| Feature | Excel | SQLite | Snowflake |
|---------|-------|--------|-----------|
| **Read Speed** | Medium | Fast | Fast |
| **Write Speed** | Slow | Fast | Medium |
| **Concurrent Users** | Poor | Good | Excellent |
| **Dataset Size** | <100K | <10M | Unlimited |
| **Setup Complexity** | None | Low | High |
| **Cost** | Free | Free | Paid |

---

## Troubleshooting

### Excel File Not Found
```
Warning: Excel file not found: C:\path\to\file.xlsx
```
**Solution:** Check that the file path in config.json is correct and the file exists.

### SQLite Table Not Found
```
Error loading from SQLite: no such table: canvas_dec_matches
```
**Solution:** Create the table first using the setup script above.

### Snowflake Connection Error
```
Error loading from Snowflake: ...
```
**Solutions:**
1. Verify credentials in config.json
2. Check that snowflake-connector-python is installed
3. Ensure your IP is whitelisted in Snowflake network policy
4. Verify warehouse is running

### Missing Columns
If your data source is missing required columns, the application will fail with a KeyError. Ensure your data source has all required columns listed above.

---

## Security Notes

### Credentials in config.json
⚠️ **Never commit config.json with real credentials to version control!**

**Best practices:**
1. Add `config.json` to `.gitignore`
2. Create `config.example.json` with placeholder values
3. Use environment variables for sensitive data:

```python
import os
import json

# Load config
with open('config.json', 'r') as f:
    config = json.load(f)

# Override with environment variables
if os.getenv('SNOWFLAKE_USER'):
    config['user'] = os.getenv('SNOWFLAKE_USER')
if os.getenv('SNOWFLAKE_PASSWORD'):
    config['password'] = os.getenv('SNOWFLAKE_PASSWORD')
```

---

## Architecture

### Files Modified
- `data_loader.py` - Data abstraction layer (new)
- `config.json` - Data source configuration (new)
- `app.py` - Updated to use data_loader module

### Key Changes
1. **Abstraction Layer:** All data loading goes through `data_loader.load_data(config)`
2. **Caching:** In-memory cache still works regardless of source
3. **Saving:** All saves go through `data_loader.save_data(df, config)`
4. **Audit Log:** Still uses SQLite for update_log (unchanged)

### Backward Compatibility
The application remains 100% compatible with the original Excel-based workflow. If you don't change the default config, it works exactly as before.
